{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c75db3-f59e-4cdf-8666-aa3f3850a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, ln, hour,abs,col\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, NGram, HashingTF, ChiSqSelector, VectorAssembler\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2caa6090-6458-40f6-9505-fb81ad43d0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 15:16:19 WARN Utils: Your hostname, Globals-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.20.10.3 instead (on interface en0)\n",
      "25/02/05 15:16:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/05 15:16:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"NYC\").master(\"local[2]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80dd950a-6f46-4e4e-bf9e-7e27c36e192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/husnusensoy/Documents/code/tt-bootcamp/.venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+---+--------------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|            ln_td| hh|         l1_distance|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+---+--------------------+\n",
      "|id2556363|        1|2016-03-26 13:11:26|2016-03-26 13:15:38|              1|-73.97377014160156|40.753997802734375|-73.97081756591797| 40.74872970581055|                 N|          252|5.529429087511423| 13|0.008220672607421875|\n",
      "|id1021173|        2|2016-03-05 21:26:54|2016-03-05 21:38:38|              1| -74.0003890991211| 40.74262619018555|-73.97667694091797|40.752830505371094|                 N|          704|6.556778356158042| 21|0.033916473388671875|\n",
      "|id0053980|        2|2016-01-10 00:01:59|2016-01-10 00:12:55|              1|-73.98748016357422| 40.71990203857422|-73.96369934082033|40.709598541259766|                 N|          656|6.486160788944089|  0|0.034084320068345164|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = spark.read.csv(\"train80.csv\",header=True,inferSchema=True)\\\n",
    "                  .withColumn(\"ln_td\", ln(\"trip_duration\"))\\\n",
    "                  .withColumn(\"hh\", hour(\"pickup_datetime\"))\\\n",
    "                  .withColumn(\"l1_distance\", abs(col(\"pickup_longitude\")-col(\"dropoff_longitude\")) + abs(col(\"pickup_latitude\")-col(\"dropoff_latitude\")) )\\\n",
    "                  .repartition(8)\n",
    "\n",
    "train.show(3)\n",
    "\n",
    "train.registerTempTable(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1e225c-6cfc-498b-85ce-ded02ab79648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|mean(trip_duration)|med_duration|\n",
      "+-------------------+------------+\n",
      "|   960.065212976095|     1166915|\n",
      "+-------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"select mean(trip_duration),count(1) as med_duration from train\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03fe8a94-456f-4e0d-80d5-8926984d57d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+-------------------+---------------+------------------+-----------------+------------------+------------------+------------------+-------------+---+--------------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|  pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration| hh|         l1_distance|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+-----------------+------------------+------------------+------------------+-------------+---+--------------------+\n",
      "|id1375364|        2|2016-02-25 08:03:18|2016-02-25 08:15:13|              2|-73.97305297851562|40.74432373046875|  -73.968505859375| 40.76462173461914|                 N|          715|  8|0.024845123291015625|\n",
      "|id3596185|        1|2016-05-30 18:29:15|2016-05-30 18:30:48|              1|-73.95536041259767|40.77701568603515|-73.95762634277344| 40.78302764892578|                 N|           93| 18|0.008277893066399145|\n",
      "|id1035847|        2|2016-03-19 19:25:33|2016-03-19 20:20:44|              1|-74.00177001953125|40.74064636230469|-73.84698486328125|40.814212799072266|                 N|         3311| 19| 0.22835159301757812|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+-----------------+------------------+------------------+------------------+-------------+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "test = spark.read.csv(\"test20.csv\",header=True,inferSchema=True)\\\n",
    "                 .withColumn(\"hh\", hour(\"pickup_datetime\"))\\\n",
    "                 .withColumn(\"l1_distance\", abs(col(\"pickup_longitude\")-col(\"dropoff_longitude\")) + abs(col(\"pickup_latitude\")-col(\"dropoff_latitude\")) )\\\n",
    "                 .repartition(8)\n",
    "\n",
    "test.show(3)\n",
    "\n",
    "test.registerTempTable(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639ff2dd-8f9e-477b-8747-3688e11d75eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              error|\n",
      "+-------------------+\n",
      "|1.030098445489977E7|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(power(trip_duration - 960,2)) as error from test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4272cc0b-da8a-4095-ab9d-82cd9992377b",
   "metadata": {},
   "source": [
    "### Some Categorical Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72af8ac5-4786-4ff6-a727-dbf3993b30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/husnusensoy/Documents/code/tt-bootcamp/.venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select vendor_id, hour(pickup_datetime) hh, mean(trip_duration) avg_dur, count(1) n \n",
    "from train \n",
    "group by 1,2\n",
    "\"\"\").registerTempTable(\"summary_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4e6cbb1-f377-4974-8ed6-bd47eb0e8f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|trip_duration|           avg_dur|\n",
      "+-------------+------------------+\n",
      "|        86346| 766.5796019900497|\n",
      "|        86286| 910.2591969020962|\n",
      "|        86365| 989.7060029665823|\n",
      "|        86357| 981.7420898329461|\n",
      "|        86353| 981.7420898329461|\n",
      "|        86348|  983.554221801336|\n",
      "|        86392|1029.2058158856762|\n",
      "|        86332| 984.1358433416688|\n",
      "|        86332| 984.1358433416688|\n",
      "|        86331| 984.1358433416688|\n",
      "|        86334| 989.6070192782995|\n",
      "|        86367|1024.2281807901861|\n",
      "|        86323|  983.554221801336|\n",
      "|        86317| 984.1358433416688|\n",
      "|        86312|  983.554221801336|\n",
      "|        86346|1024.2281807901861|\n",
      "|        86303| 981.7420898329461|\n",
      "|        86301|  983.554221801336|\n",
      "|        86307| 989.7060029665823|\n",
      "|        86304| 989.6070192782995|\n",
      "+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select a.trip_duration,b.avg_dur \n",
    "from test a \n",
    "join summary_stats b on (a.vendor_id = b.vendor_id and hour(a.pickup_datetime) = b.hh) \n",
    "order by abs(a.trip_duration - avg_dur) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5625894a-b7bb-4322-a506-ac5fdbefaa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|avg(pow((trip_duration - avg_dur), 2))|\n",
      "+--------------------------------------+\n",
      "|                   1.028232263265131E7|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select avg(pow(a.trip_duration - b.avg_dur ,2))\n",
    "from test a \n",
    "join summary_stats b on (a.vendor_id = b.vendor_id and hour(a.pickup_datetime) = b.hh) \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc79aa2-0434-4bad-af2e-0ea8c89964c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression,RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdaa6e51-66c9-4f4c-b538-c53c59fbb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = Pipeline(stages=[StringIndexer(inputCol=\"vendor_id\", outputCol=\"vendor_id_idx\")\n",
    "                       ,VectorAssembler(inputCols=[\"l1_distance\", \"vendor_id_idx\",\"hh\",\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\"], outputCol=\"features\"),\n",
    "                     RandomForestRegressor(labelCol=\"ln_td\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91dd0ddd-d30f-4811-bd32-5f34f651b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "ppl_model = ppl.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36108c6a-9a12-42cd-8373-fb71d0cbb1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/husnusensoy/Documents/code/tt-bootcamp/.venv/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.4.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "ppl_model.transform(test).select(\"trip_duration\", \"prediction\").registerTempTable(\"first_ol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdfb1899-fdc8-435f-8117-b23eb312c748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|trip_duration|   EXP(prediction)|\n",
      "+-------------+------------------+\n",
      "|        86332|251.36924985036353|\n",
      "|        86286|216.77997786638704|\n",
      "|        86321|263.46536359092715|\n",
      "|        86356| 305.4719633978441|\n",
      "|        86285|241.26000668173543|\n",
      "|        86325|281.78016688928653|\n",
      "|        86339|  300.883486631336|\n",
      "|        86236|204.28203221361485|\n",
      "|        86241|212.00421859855277|\n",
      "|        86334|309.08883002425114|\n",
      "+-------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_duration,exp(prediction) from  first_ol order by abs(trip_duration-exp(prediction)) desc \").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16149707-c02e-49f8-9d80-1f7f14bd5177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|SQRT(avg(POWER((trip_duration - EXP(prediction)), 2)))|\n",
      "+------------------------------------------------------+\n",
      "|                                     3170.175473915946|\n",
      "+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sqrt(avg(power(trip_duration-exp(prediction),2))) from  first_ol \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eed058-167d-4991-bf7c-e62685e6268f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
